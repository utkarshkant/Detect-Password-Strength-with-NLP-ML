{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Password Strength\n",
    "### Objective : Create a model to detect the strength of any password.\n",
    "- This will be solved using **Multiclass Classification NLP and Machine Learning**\n",
    "- Password strength is determined by multiple factors. Some of them are:\n",
    "    - Length of the password\n",
    "    - Choice of characters used\n",
    "    - Combination of the chosen characters\n",
    "    - and more . . . \n",
    "- Here we will apply NLP and each character in the password will prove essential to our analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import neccessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_row', 1000)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# configurations\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 2810: expected 2 fields, saw 5\\nSkipping line 4641: expected 2 fields, saw 5\\nSkipping line 7171: expected 2 fields, saw 5\\nSkipping line 11220: expected 2 fields, saw 5\\nSkipping line 13809: expected 2 fields, saw 5\\nSkipping line 14132: expected 2 fields, saw 5\\nSkipping line 14293: expected 2 fields, saw 5\\nSkipping line 14865: expected 2 fields, saw 5\\nSkipping line 17419: expected 2 fields, saw 5\\nSkipping line 22801: expected 2 fields, saw 5\\nSkipping line 25001: expected 2 fields, saw 5\\nSkipping line 26603: expected 2 fields, saw 5\\nSkipping line 26742: expected 2 fields, saw 5\\nSkipping line 29702: expected 2 fields, saw 5\\nSkipping line 32767: expected 2 fields, saw 5\\nSkipping line 32878: expected 2 fields, saw 5\\nSkipping line 35643: expected 2 fields, saw 5\\nSkipping line 36550: expected 2 fields, saw 5\\nSkipping line 38732: expected 2 fields, saw 5\\nSkipping line 40567: expected 2 fields, saw 5\\nSkipping line 40576: expected 2 fields, saw 5\\nSkipping line 41864: expected 2 fields, saw 5\\nSkipping line 46861: expected 2 fields, saw 5\\nSkipping line 47939: expected 2 fields, saw 5\\nSkipping line 48628: expected 2 fields, saw 5\\nSkipping line 48908: expected 2 fields, saw 5\\nSkipping line 57582: expected 2 fields, saw 5\\nSkipping line 58782: expected 2 fields, saw 5\\nSkipping line 58984: expected 2 fields, saw 5\\nSkipping line 61518: expected 2 fields, saw 5\\nSkipping line 63451: expected 2 fields, saw 5\\nSkipping line 68141: expected 2 fields, saw 5\\nSkipping line 72083: expected 2 fields, saw 5\\nSkipping line 74027: expected 2 fields, saw 5\\nSkipping line 77811: expected 2 fields, saw 5\\nSkipping line 83958: expected 2 fields, saw 5\\nSkipping line 85295: expected 2 fields, saw 5\\nSkipping line 88665: expected 2 fields, saw 5\\nSkipping line 89198: expected 2 fields, saw 5\\nSkipping line 92499: expected 2 fields, saw 5\\nSkipping line 92751: expected 2 fields, saw 5\\nSkipping line 93689: expected 2 fields, saw 5\\nSkipping line 94776: expected 2 fields, saw 5\\nSkipping line 97334: expected 2 fields, saw 5\\nSkipping line 102316: expected 2 fields, saw 5\\nSkipping line 103421: expected 2 fields, saw 5\\nSkipping line 106872: expected 2 fields, saw 5\\nSkipping line 109363: expected 2 fields, saw 5\\nSkipping line 110117: expected 2 fields, saw 5\\nSkipping line 110465: expected 2 fields, saw 5\\nSkipping line 113843: expected 2 fields, saw 5\\nSkipping line 115634: expected 2 fields, saw 5\\nSkipping line 121518: expected 2 fields, saw 5\\nSkipping line 123692: expected 2 fields, saw 5\\nSkipping line 124708: expected 2 fields, saw 5\\nSkipping line 129608: expected 2 fields, saw 5\\nSkipping line 133176: expected 2 fields, saw 5\\nSkipping line 135532: expected 2 fields, saw 5\\nSkipping line 138042: expected 2 fields, saw 5\\nSkipping line 139485: expected 2 fields, saw 5\\nSkipping line 140401: expected 2 fields, saw 5\\nSkipping line 144093: expected 2 fields, saw 5\\nSkipping line 149850: expected 2 fields, saw 5\\nSkipping line 151831: expected 2 fields, saw 5\\nSkipping line 158014: expected 2 fields, saw 5\\nSkipping line 162047: expected 2 fields, saw 5\\nSkipping line 164515: expected 2 fields, saw 5\\nSkipping line 170313: expected 2 fields, saw 5\\nSkipping line 171325: expected 2 fields, saw 5\\nSkipping line 171424: expected 2 fields, saw 5\\nSkipping line 175920: expected 2 fields, saw 5\\nSkipping line 176210: expected 2 fields, saw 5\\nSkipping line 183603: expected 2 fields, saw 5\\nSkipping line 190264: expected 2 fields, saw 5\\nSkipping line 191683: expected 2 fields, saw 5\\nSkipping line 191988: expected 2 fields, saw 5\\nSkipping line 195450: expected 2 fields, saw 5\\nSkipping line 195754: expected 2 fields, saw 5\\nSkipping line 197124: expected 2 fields, saw 5\\nSkipping line 199263: expected 2 fields, saw 5\\nSkipping line 202603: expected 2 fields, saw 5\\nSkipping line 209960: expected 2 fields, saw 5\\nSkipping line 213218: expected 2 fields, saw 5\\nSkipping line 217060: expected 2 fields, saw 5\\nSkipping line 220121: expected 2 fields, saw 5\\nSkipping line 223518: expected 2 fields, saw 5\\nSkipping line 226293: expected 2 fields, saw 5\\nSkipping line 227035: expected 2 fields, saw 7\\nSkipping line 227341: expected 2 fields, saw 5\\nSkipping line 227808: expected 2 fields, saw 5\\nSkipping line 228516: expected 2 fields, saw 5\\nSkipping line 228733: expected 2 fields, saw 5\\nSkipping line 232043: expected 2 fields, saw 5\\nSkipping line 232426: expected 2 fields, saw 5\\nSkipping line 234490: expected 2 fields, saw 5\\nSkipping line 239626: expected 2 fields, saw 5\\nSkipping line 240461: expected 2 fields, saw 5\\nSkipping line 244518: expected 2 fields, saw 5\\nSkipping line 245395: expected 2 fields, saw 5\\nSkipping line 246168: expected 2 fields, saw 5\\nSkipping line 246655: expected 2 fields, saw 5\\nSkipping line 246752: expected 2 fields, saw 5\\nSkipping line 247189: expected 2 fields, saw 5\\nSkipping line 250276: expected 2 fields, saw 5\\nSkipping line 255327: expected 2 fields, saw 5\\nSkipping line 257094: expected 2 fields, saw 5\\n'\n",
      "b'Skipping line 264626: expected 2 fields, saw 5\\nSkipping line 265028: expected 2 fields, saw 5\\nSkipping line 269150: expected 2 fields, saw 5\\nSkipping line 271360: expected 2 fields, saw 5\\nSkipping line 273975: expected 2 fields, saw 5\\nSkipping line 274742: expected 2 fields, saw 5\\nSkipping line 276227: expected 2 fields, saw 5\\nSkipping line 279807: expected 2 fields, saw 5\\nSkipping line 283425: expected 2 fields, saw 5\\nSkipping line 287468: expected 2 fields, saw 5\\nSkipping line 292995: expected 2 fields, saw 5\\nSkipping line 293496: expected 2 fields, saw 5\\nSkipping line 293735: expected 2 fields, saw 5\\nSkipping line 295060: expected 2 fields, saw 5\\nSkipping line 296643: expected 2 fields, saw 5\\nSkipping line 296848: expected 2 fields, saw 5\\nSkipping line 308926: expected 2 fields, saw 5\\nSkipping line 310360: expected 2 fields, saw 5\\nSkipping line 317004: expected 2 fields, saw 5\\nSkipping line 318207: expected 2 fields, saw 5\\nSkipping line 331783: expected 2 fields, saw 5\\nSkipping line 333864: expected 2 fields, saw 5\\nSkipping line 335958: expected 2 fields, saw 5\\nSkipping line 336290: expected 2 fields, saw 5\\nSkipping line 343526: expected 2 fields, saw 5\\nSkipping line 343857: expected 2 fields, saw 5\\nSkipping line 344059: expected 2 fields, saw 5\\nSkipping line 348691: expected 2 fields, saw 5\\nSkipping line 353446: expected 2 fields, saw 5\\nSkipping line 357073: expected 2 fields, saw 5\\nSkipping line 359753: expected 2 fields, saw 5\\nSkipping line 359974: expected 2 fields, saw 5\\nSkipping line 366534: expected 2 fields, saw 5\\nSkipping line 369514: expected 2 fields, saw 5\\nSkipping line 377759: expected 2 fields, saw 5\\nSkipping line 379327: expected 2 fields, saw 5\\nSkipping line 380769: expected 2 fields, saw 5\\nSkipping line 381073: expected 2 fields, saw 5\\nSkipping line 381489: expected 2 fields, saw 5\\nSkipping line 386304: expected 2 fields, saw 5\\nSkipping line 387635: expected 2 fields, saw 5\\nSkipping line 389613: expected 2 fields, saw 5\\nSkipping line 392604: expected 2 fields, saw 5\\nSkipping line 393184: expected 2 fields, saw 5\\nSkipping line 395530: expected 2 fields, saw 5\\nSkipping line 396939: expected 2 fields, saw 5\\nSkipping line 397385: expected 2 fields, saw 5\\nSkipping line 397509: expected 2 fields, saw 5\\nSkipping line 402902: expected 2 fields, saw 5\\nSkipping line 405187: expected 2 fields, saw 5\\nSkipping line 408412: expected 2 fields, saw 5\\nSkipping line 419423: expected 2 fields, saw 5\\nSkipping line 420962: expected 2 fields, saw 5\\nSkipping line 425965: expected 2 fields, saw 5\\nSkipping line 427496: expected 2 fields, saw 5\\nSkipping line 438881: expected 2 fields, saw 5\\nSkipping line 439776: expected 2 fields, saw 5\\nSkipping line 440345: expected 2 fields, saw 5\\nSkipping line 445507: expected 2 fields, saw 5\\nSkipping line 445548: expected 2 fields, saw 5\\nSkipping line 447184: expected 2 fields, saw 5\\nSkipping line 448603: expected 2 fields, saw 5\\nSkipping line 451732: expected 2 fields, saw 5\\nSkipping line 458249: expected 2 fields, saw 5\\nSkipping line 460274: expected 2 fields, saw 5\\nSkipping line 467630: expected 2 fields, saw 5\\nSkipping line 473961: expected 2 fields, saw 5\\nSkipping line 476281: expected 2 fields, saw 5\\nSkipping line 478010: expected 2 fields, saw 5\\nSkipping line 478322: expected 2 fields, saw 5\\nSkipping line 479999: expected 2 fields, saw 5\\nSkipping line 480898: expected 2 fields, saw 5\\nSkipping line 481688: expected 2 fields, saw 5\\nSkipping line 485193: expected 2 fields, saw 5\\nSkipping line 485519: expected 2 fields, saw 5\\nSkipping line 486000: expected 2 fields, saw 5\\nSkipping line 489063: expected 2 fields, saw 5\\nSkipping line 494525: expected 2 fields, saw 5\\nSkipping line 495009: expected 2 fields, saw 5\\nSkipping line 501954: expected 2 fields, saw 5\\nSkipping line 508035: expected 2 fields, saw 5\\nSkipping line 508828: expected 2 fields, saw 5\\nSkipping line 509833: expected 2 fields, saw 5\\nSkipping line 510410: expected 2 fields, saw 5\\nSkipping line 518229: expected 2 fields, saw 5\\nSkipping line 520302: expected 2 fields, saw 5\\nSkipping line 520340: expected 2 fields, saw 5\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 525174: expected 2 fields, saw 5\\nSkipping line 526251: expected 2 fields, saw 5\\nSkipping line 529611: expected 2 fields, saw 5\\nSkipping line 531398: expected 2 fields, saw 5\\nSkipping line 534146: expected 2 fields, saw 5\\nSkipping line 544954: expected 2 fields, saw 5\\nSkipping line 553002: expected 2 fields, saw 5\\nSkipping line 553883: expected 2 fields, saw 5\\nSkipping line 553887: expected 2 fields, saw 5\\nSkipping line 553915: expected 2 fields, saw 5\\nSkipping line 554172: expected 2 fields, saw 5\\nSkipping line 563534: expected 2 fields, saw 5\\nSkipping line 565191: expected 2 fields, saw 5\\nSkipping line 574108: expected 2 fields, saw 5\\nSkipping line 574412: expected 2 fields, saw 5\\nSkipping line 575985: expected 2 fields, saw 5\\nSkipping line 580091: expected 2 fields, saw 5\\nSkipping line 582682: expected 2 fields, saw 5\\nSkipping line 585885: expected 2 fields, saw 5\\nSkipping line 590171: expected 2 fields, saw 5\\nSkipping line 591924: expected 2 fields, saw 5\\nSkipping line 592515: expected 2 fields, saw 5\\nSkipping line 593888: expected 2 fields, saw 5\\nSkipping line 596245: expected 2 fields, saw 5\\nSkipping line 607344: expected 2 fields, saw 5\\nSkipping line 607633: expected 2 fields, saw 5\\nSkipping line 610939: expected 2 fields, saw 5\\nSkipping line 613638: expected 2 fields, saw 5\\nSkipping line 615643: expected 2 fields, saw 5\\nSkipping line 615901: expected 2 fields, saw 5\\nSkipping line 617389: expected 2 fields, saw 5\\nSkipping line 634641: expected 2 fields, saw 5\\nSkipping line 635755: expected 2 fields, saw 5\\nSkipping line 646243: expected 2 fields, saw 5\\nSkipping line 647165: expected 2 fields, saw 5\\nSkipping line 648610: expected 2 fields, saw 5\\nSkipping line 648772: expected 2 fields, saw 5\\nSkipping line 651833: expected 2 fields, saw 5\\nSkipping line 653663: expected 2 fields, saw 5\\nSkipping line 656233: expected 2 fields, saw 5\\nSkipping line 656694: expected 2 fields, saw 5\\nSkipping line 659783: expected 2 fields, saw 5\\nSkipping line 660478: expected 2 fields, saw 5\\nSkipping line 661133: expected 2 fields, saw 5\\nSkipping line 661736: expected 2 fields, saw 5\\nSkipping line 669827: expected 2 fields, saw 5\\n'\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv('data.csv',',',error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `','` - comma separated value\n",
    "- `error_bad_lines=False` - skips the rows that are not properly comma-separated values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>password</th>\n",
       "      <th>strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>kzde5577</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>kino3434</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>visi7k1yr</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>megzy123</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>lamborghin1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      password  strength\n",
       "0     kzde5577         1\n",
       "1     kino3434         1\n",
       "2    visi7k1yr         1\n",
       "3     megzy123         1\n",
       "4  lamborghin1         1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Rows    :  669640\n",
      "# Columns :  2\n"
     ]
    }
   ],
   "source": [
    "# dimensions\n",
    "print('# Rows    : ', df.shape[0])\n",
    "print('# Columns : ', df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 669640 entries, 0 to 669639\n",
      "Data columns (total 2 columns):\n",
      "password    669639 non-null object\n",
      "strength    669640 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 10.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# metadata - features\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1 missing record in the `password` feature of the dataset\n",
    "- which we will treat in future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "password\n",
      "strength\n"
     ]
    }
   ],
   "source": [
    "# features\n",
    "for i in df.columns: print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Dictionary\n",
    "- `password` contains all the passwords that are a combination of alphabets, numbers and special characters\n",
    "- `strength` is the dependent variable\n",
    "- `strength`\n",
    "    - 0 means weak password\n",
    "    - 1 means strong password\n",
    "    - 2 means very strong password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "password    669639\n",
       "strength         3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset composition\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    496801\n",
       "0     89702\n",
       "2     83137\n",
       "Name: strength, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# value counts\n",
    "df['strength'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x17782b78088>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# count plot\n",
    "sns.countplot(x='strength', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `strength = 1` has the most number of entries in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Delete null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "password    1\n",
       "strength    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>password</th>\n",
       "      <th>strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>367579</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       password  strength\n",
       "367579      NaN         0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# record containing null value\n",
    "df[df['password'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop null record\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Rows    :  669639\n",
      "# Columns :  2\n"
     ]
    }
   ],
   "source": [
    "# dimensions\n",
    "    # Rows    :  669640\n",
    "    # Columns :  2\n",
    "print('# Rows    : ', df.shape[0])\n",
    "print('# Columns : ', df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- null value successfully dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tuple of all passwords and their strength\n",
    "pwd_tuple = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['kzde5577', 1],\n",
       "       ['kino3434', 1],\n",
       "       ['visi7k1yr', 1],\n",
       "       ...,\n",
       "       ['184520socram', 1],\n",
       "       ['marken22a', 1],\n",
       "       ['fxx4pw4g', 1]], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview\n",
    "pwd_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshuffle the password since all records have been aligned based on their strengths\n",
    "import random\n",
    "random.shuffle(pwd_tuple)  # shuffle randomly for robustnes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['kzde5577', 1],\n",
       "       ['kzde5577', 1],\n",
       "       ['kino3434', 1],\n",
       "       ...,\n",
       "       ['sinena193', 1],\n",
       "       ['otexyqy837', 1],\n",
       "       ['wpkatgyxz2', 1]], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview\n",
    "pwd_tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset based on dependent and independent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependent feature\n",
    "y = [d[1] for d in pwd_tuple]\n",
    "\n",
    "# independent feature\n",
    "X = [d[0] for d in pwd_tuple]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kzde5577',\n",
       " 'kzde5577',\n",
       " 'kino3434',\n",
       " 'megzy123',\n",
       " 'megzy123',\n",
       " 'kino3434',\n",
       " 'AVYq1lDE4MgAZfNt',\n",
       " 'megzy123',\n",
       " 'kino3434',\n",
       " 'visi7k1yr',\n",
       " 'kzde5577',\n",
       " 'v1118714',\n",
       " '612035180tok',\n",
       " 'kzde5577',\n",
       " 'lamborghin1',\n",
       " 'asv5o9yu',\n",
       " 'u6c8vhow',\n",
       " 'u6c8vhow',\n",
       " 'idofo673',\n",
       " 'asv5o9yu',\n",
       " 'as326159',\n",
       " 'lamborghin1',\n",
       " '6975038lp',\n",
       " '52558000aaa',\n",
       " 'as326159',\n",
       " 'kino3434',\n",
       " '6975038lp',\n",
       " 'kino3434',\n",
       " 'asv5o9yu',\n",
       " 'schalke04',\n",
       " '612035180tok',\n",
       " 'kino3434',\n",
       " 'schalke04',\n",
       " 'sbl571017',\n",
       " 'memjan123',\n",
       " 'intel1',\n",
       " '6975038lp',\n",
       " 'fahad123',\n",
       " 'schalke04',\n",
       " 'as326159',\n",
       " 'exitos2009',\n",
       " 'megzy123',\n",
       " 'megzy123',\n",
       " 'intel1',\n",
       " 'sbl571017',\n",
       " 'kzde5577',\n",
       " 'jytifok873',\n",
       " 'a2531106',\n",
       " 'intel1',\n",
       " 'visi7k1yr',\n",
       " 'asgaliu11',\n",
       " 'lamborghin1',\n",
       " 'fahad123',\n",
       " 'g067057895',\n",
       " 'ok>bdk',\n",
       " 'hpqkoxsn5',\n",
       " 'trabajonet9',\n",
       " 'pHyqueDIyNQ8vmhb',\n",
       " 'idofo673',\n",
       " 'gaymaids1',\n",
       " 'patri1973',\n",
       " 'prisonbreak1',\n",
       " '52558000aaa',\n",
       " 'AVYq1lDE4MgAZfNt',\n",
       " 'memjan123',\n",
       " 'd04m11',\n",
       " 'idofo673',\n",
       " 'elyass15@ajilent-ci',\n",
       " 'universe2908',\n",
       " 'yk530mg8',\n",
       " 'as326159',\n",
       " 'sbl571017',\n",
       " 'yitbos77',\n",
       " 'exitos2009',\n",
       " 'il0vey0u',\n",
       " 'yqugu927',\n",
       " 'AVYq1lDE4MgAZfNt',\n",
       " 'hpqkoxsn5',\n",
       " 'a2531106',\n",
       " 'matiofox08',\n",
       " 'yitbos77',\n",
       " 'tamanagung6',\n",
       " 'kino3434',\n",
       " 'fahad123',\n",
       " 'jytifok873',\n",
       " 'jerusalem393',\n",
       " 'lamborghin1',\n",
       " 'Iamthelegend1!',\n",
       " 'megzy123',\n",
       " 'olmaz.',\n",
       " 'asv5o9yu',\n",
       " 'cesarmaio1',\n",
       " 'yqugu927',\n",
       " 'pHyqueDIyNQ8vmhb',\n",
       " 'jalingo1',\n",
       " 'p2share',\n",
       " 'kzde5577',\n",
       " 'trabajonet9',\n",
       " 'AVYq1lDE4MgAZfNt',\n",
       " 'faranumar91',\n",
       " 'juliel009',\n",
       " 'kVczcljg4OA25Aeb',\n",
       " 'jonothepoop1',\n",
       " 'k9b8cz6aj2',\n",
       " 'asv5o9yu',\n",
       " 'k1k2k3k4k5k6',\n",
       " 'trabajonet9',\n",
       " 's4m2dx9e6',\n",
       " 'obstacle25',\n",
       " 'czuodhj972',\n",
       " 'prisonbreak1',\n",
       " 'mmm23mm',\n",
       " '3vszncp4',\n",
       " 'ass359',\n",
       " 'jonothepoop1',\n",
       " 'memjan123',\n",
       " 'gill02',\n",
       " 'xyws951753',\n",
       " 'megzy123',\n",
       " 'g067057895',\n",
       " 'klara-tershina3H',\n",
       " 'znbl5tj1',\n",
       " 'snolyuj04',\n",
       " 'fk9qi21m',\n",
       " 'yllime123',\n",
       " 'kVczcljg4OA25Aeb',\n",
       " '123maxbala',\n",
       " 'yllime123',\n",
       " 'universe2908',\n",
       " 'olmaz.',\n",
       " 'yk530mg8',\n",
       " 'asv5o9yu',\n",
       " 'intel1',\n",
       " 'ikanez886',\n",
       " 'ga98SIzk0NwhiZaE',\n",
       " 'pato221182',\n",
       " '123477889a',\n",
       " 'xiau5ff',\n",
       " 'yitbos77',\n",
       " 'warriors08',\n",
       " 'kswa2mrv',\n",
       " 'ass359',\n",
       " 'lamborghin1',\n",
       " 'ass359',\n",
       " 'znbl5tj1',\n",
       " 'pHyqueDIyNQ8vmhb',\n",
       " 'zoobike04',\n",
       " 'znbl5tj1',\n",
       " 'mmm23mm',\n",
       " 'asgaliu11',\n",
       " 'elyass15@ajilent-ci',\n",
       " 'w9209640',\n",
       " 'rogyh820',\n",
       " 'kVczcljg4OA25Aeb',\n",
       " 'kswa2mrv',\n",
       " 'p@sslng2diword',\n",
       " 'gdfn76',\n",
       " 'kinga22',\n",
       " 'kino3434',\n",
       " 'v1118714',\n",
       " 'RPFUOUDQwMwVW0AS',\n",
       " 'WUt9IZzE0OQ7PkNE',\n",
       " 'ikanez886',\n",
       " 'gill02',\n",
       " 'xyws951753',\n",
       " 'klara-tershina3H',\n",
       " 'poseidon2011',\n",
       " 'ga98SIzk0NwhiZaE',\n",
       " 'ass359',\n",
       " 'gaymaids1',\n",
       " '123net123',\n",
       " 'gdfn76',\n",
       " 'go7kew7a2po',\n",
       " '2010server',\n",
       " 'hayhayq2',\n",
       " 'WUt9IZzE0OQ7PkNE',\n",
       " 'w9209640',\n",
       " 'p2share',\n",
       " 'farrukhcse12',\n",
       " 'vehat387',\n",
       " 'yllime123',\n",
       " 'AVYq1lDE4MgAZfNt',\n",
       " 'tin030201',\n",
       " '147963asd',\n",
       " 'faranumar91',\n",
       " 'gill02',\n",
       " 'bozoxik602',\n",
       " 'yllime123',\n",
       " 'kunyukbabi69',\n",
       " 'k9b8cz6aj2',\n",
       " 'zgmfnwuq25',\n",
       " 'faranumar91',\n",
       " '3vszncp4',\n",
       " 'fk9qi21m',\n",
       " '6975038lp',\n",
       " '0870330135a',\n",
       " '6tequila6',\n",
       " '3vszncp4',\n",
       " 'poseidon2011',\n",
       " 'yqugu927',\n",
       " 'wisal1234',\n",
       " 'RqsuUsDYxNgr8T40',\n",
       " 'as326159',\n",
       " 'ok>bdk',\n",
       " '20Dgw7TQ0OQVdly7',\n",
       " 'v1118714',\n",
       " 'wxS2ztDk4OATjBfI',\n",
       " 'vehat387',\n",
       " '52558000aaa',\n",
       " '2021848709.',\n",
       " '123maxbala',\n",
       " 'kVczcljg4OA25Aeb',\n",
       " 'meopvywk628',\n",
       " 'www32223222',\n",
       " 'fahad123',\n",
       " 'okn9zp9o',\n",
       " 'u6c8vhow',\n",
       " 'yut0838828185',\n",
       " 'j09000',\n",
       " 'nK0yKXTU0NQHZE2e',\n",
       " '147963asd',\n",
       " '52558000aaa',\n",
       " 'znbl5tj1',\n",
       " 'znbl5tj1',\n",
       " 'x8512514',\n",
       " 'pHyqueDIyNQ8vmhb',\n",
       " 'nK0yKXTU0NQHZE2e',\n",
       " 'x8512514',\n",
       " 'jytifok873',\n",
       " 'ldteugao6',\n",
       " 'yuri110995',\n",
       " '123477889a',\n",
       " 'ns2b0727',\n",
       " 'edcmki90',\n",
       " '6tequila6',\n",
       " 'sknq7m0',\n",
       " '52558000aaa',\n",
       " 'sarahi1628',\n",
       " '0870330135a',\n",
       " 'gtlek',\n",
       " '1972vishara',\n",
       " 'omakiva153',\n",
       " 'finisterra1',\n",
       " 'wisal1234',\n",
       " 'klara-tershina3H',\n",
       " '283671gus',\n",
       " 's4m2dx9e6',\n",
       " 'change201',\n",
       " 'z888888',\n",
       " 'c3h8bkzr',\n",
       " 'prisonbreak1',\n",
       " 'uxyloga692',\n",
       " 'caramelo9',\n",
       " 'alimagik1',\n",
       " 'cesarmaio1',\n",
       " 'jonothepoop1',\n",
       " '1972vishara',\n",
       " 'owote852',\n",
       " 'e667794c1d',\n",
       " 'olmaz.',\n",
       " 'atigi839',\n",
       " 'yuri110995',\n",
       " 'kry1z9',\n",
       " 'visi7k1yr',\n",
       " '4osxw4r',\n",
       " 'kinga22',\n",
       " 'roxana1993',\n",
       " 'ns2b0727',\n",
       " '20Dgw7TQ0OQVdly7',\n",
       " 'omakiva153',\n",
       " 'rLLh4WDQ2OAWbDO5',\n",
       " 'abizar08',\n",
       " 'tia150979',\n",
       " 'kry1z9',\n",
       " 'atigi839',\n",
       " '4fqa52vecr',\n",
       " 'oekojWyH120063',\n",
       " 'e667794c1d',\n",
       " 'lsdlsd1',\n",
       " 'Iamthelegend1!',\n",
       " 'teste10',\n",
       " 'wycinu436',\n",
       " 'njmania114',\n",
       " 'rLLh4WDQ2OAWbDO5',\n",
       " 'megzy123',\n",
       " 'ubojig109',\n",
       " 'lzhzad1989',\n",
       " 'teste10',\n",
       " 'pato221182',\n",
       " 'cesarmaio1',\n",
       " 'jalal123456',\n",
       " 'il0vey0u',\n",
       " 'a2531106',\n",
       " 'icap12',\n",
       " 'metopelo1623',\n",
       " 'sarahi1628',\n",
       " 'nicolas05',\n",
       " 'polo2014',\n",
       " 'demon10',\n",
       " 'kzde5577',\n",
       " '2021848709.',\n",
       " 'ass359',\n",
       " 'afs34214',\n",
       " 'finisterra1',\n",
       " 'farrukhcse12',\n",
       " 'g3rappa',\n",
       " 'il0vey0u',\n",
       " 'ubojig109',\n",
       " 'TyWM72UNEex8Q8Y',\n",
       " 'kswa2mrv',\n",
       " 'teemteem97',\n",
       " 'ubojig109',\n",
       " 'barra23',\n",
       " 'yu4cmn',\n",
       " 'xanyrum650',\n",
       " 'lrhxmevb620',\n",
       " 'bgrvl80',\n",
       " 's0xwym7h',\n",
       " 'asgaliu11',\n",
       " 'colorado27',\n",
       " 'tamanagung6',\n",
       " 'xiau5ff',\n",
       " 'mazdarx7',\n",
       " 'QWERTY0011',\n",
       " 'YADHJIGSAWS11',\n",
       " 'yllime123',\n",
       " 'rogyh820',\n",
       " 'demon10',\n",
       " 'exitos2009',\n",
       " 'ejeko677',\n",
       " 'jalingo1',\n",
       " 'yk530mg8',\n",
       " 'x0004534',\n",
       " 'cesarmaio1',\n",
       " 'ram@!sita15392',\n",
       " 's4m2dx9e6',\n",
       " 'ajyrew547',\n",
       " 'warriors08',\n",
       " 'wxS2ztDk4OATjBfI',\n",
       " 'khurram_',\n",
       " 'trabajonet9',\n",
       " 'mayur@8netinfotech',\n",
       " 'ga98SIzk0NwhiZaE',\n",
       " 'gkrqjs6',\n",
       " '123maxbala',\n",
       " 'asgaliu11',\n",
       " 'khaled12',\n",
       " '07dpv1127b',\n",
       " 'znbl5tj1',\n",
       " 'wuzyci421',\n",
       " 'mathilde54550',\n",
       " 'xlxlxl777',\n",
       " 'denise18',\n",
       " 'wibi182d',\n",
       " 'gdfn76',\n",
       " 'wycinu436',\n",
       " 'yut0838828185',\n",
       " 'ginger972',\n",
       " 'WUt9IZzE0OQ7PkNE',\n",
       " 'idofo673',\n",
       " 'jEzZXUTE3MgJ4fVk',\n",
       " 'barboza221294',\n",
       " '4osxw4r',\n",
       " 'ubojig109',\n",
       " 'obstacle25',\n",
       " 'mazdarx7',\n",
       " 'z7zbgIDkzMQeHUd9',\n",
       " '838188linh',\n",
       " 'enziitoo1234',\n",
       " 'jalal123456',\n",
       " '6yy6yy',\n",
       " 'kVczcljg4OA25Aeb',\n",
       " '123477889a',\n",
       " 'caramelo9',\n",
       " '123nicole',\n",
       " 'djngeyut2707',\n",
       " 'ubojig109',\n",
       " 'nK0yKXTU0NQHZE2e',\n",
       " '123477889a',\n",
       " '283671gus',\n",
       " 'webstudio8',\n",
       " 'xanyrum650',\n",
       " 'pikey231',\n",
       " 'hayhayq2',\n",
       " 'afs34214',\n",
       " '64959rodro',\n",
       " 'UF1Z2WjE5Mg26R1K',\n",
       " 'desmondkok21',\n",
       " '123net123',\n",
       " 'w9209640',\n",
       " '1597535youssi',\n",
       " 'buqodym199',\n",
       " 'samael666',\n",
       " 'desmondkok21',\n",
       " 'qn5xpg3k00',\n",
       " 'IjUcOtYqAwel725',\n",
       " '2fakjv',\n",
       " 'uxyloga692',\n",
       " '2akira2',\n",
       " 'a0972986650',\n",
       " 'd6VyrkFV6oblxNs5N8cW',\n",
       " 'buqodym199',\n",
       " 'VMjz4eTkxNAbOyUU',\n",
       " '215466kenyi',\n",
       " 'sofietou74',\n",
       " 'teemteem97',\n",
       " 'folashade1',\n",
       " 'rLLh4WDQ2OAWbDO5',\n",
       " '6tequila6',\n",
       " 'afs34214',\n",
       " 'mayur@8netinfotech',\n",
       " 'bellsuki1',\n",
       " 'rakag279',\n",
       " '123maxbala',\n",
       " 'sarahi1628',\n",
       " 'barra23',\n",
       " 'pato221182',\n",
       " 'krumbul123',\n",
       " 'lrhxmevb620',\n",
       " 'desmondkok21',\n",
       " 'g067057895',\n",
       " 'ebogel225',\n",
       " 'hpqkoxsn5',\n",
       " 'khaled12',\n",
       " 'shotiko18',\n",
       " 'khaled12',\n",
       " 'fahad123',\n",
       " 'jonothepoop1',\n",
       " 'tim80327',\n",
       " 'w9209640',\n",
       " 'v1118714',\n",
       " 'witek1709',\n",
       " '07dpv1127b',\n",
       " 'cigicigi123',\n",
       " 'sysoja794',\n",
       " 'z888888',\n",
       " 'trabajonet9',\n",
       " 'kVczcljg4OA25Aeb',\n",
       " 'Oshity07142014',\n",
       " 'viri13',\n",
       " 'kino3434',\n",
       " 'k9b8cz6aj2',\n",
       " 'gkrqjs6',\n",
       " '3y6iwef2g6',\n",
       " '1qa2ws3ed4rf',\n",
       " 'owote852',\n",
       " 'hayhayq2',\n",
       " '1972vishara',\n",
       " 'kah4544875',\n",
       " 'q0pv0fk',\n",
       " 'krumbul123',\n",
       " 'galoucura1',\n",
       " 'b4NbTxDEyNgG141J',\n",
       " 'alchimie79',\n",
       " 'woon12',\n",
       " 'vietnga92',\n",
       " '26522876p',\n",
       " 'g067057895',\n",
       " '07dpv1127b',\n",
       " 'cigicigi123',\n",
       " 'ebogel225',\n",
       " 'sarahi1628',\n",
       " 'tia150979',\n",
       " 'rogyh820',\n",
       " 'wisal1234',\n",
       " 'calcifer32',\n",
       " 'elabadmin1386',\n",
       " 'u6c8vhow',\n",
       " 'lsdlsd1',\n",
       " 'klara-tershina3H',\n",
       " 'rogyh820',\n",
       " 'gvczfel801',\n",
       " 'bugatti01',\n",
       " 'aquhih220',\n",
       " 'YADHJIGSAWS11',\n",
       " 'lzhzad1989',\n",
       " 'bgrvl80',\n",
       " 'mazdarx7',\n",
       " 'sd6x9s3s',\n",
       " 'as326159',\n",
       " 'kyodai666',\n",
       " 'wibi182d',\n",
       " 'ikanez886',\n",
       " '147963asd',\n",
       " 'terrassa6',\n",
       " '9950twofour0',\n",
       " 'pato221182',\n",
       " '1991vikash',\n",
       " 'polo2014',\n",
       " '101010hadis',\n",
       " 'uxyloga692',\n",
       " 'patri1973',\n",
       " 'sebo82',\n",
       " 'acetita478',\n",
       " 'paladinas1',\n",
       " 'JEQuloqOFUd102',\n",
       " 'senghong2009',\n",
       " 'bellsuki1',\n",
       " 'IRZA98',\n",
       " '1597535youssi',\n",
       " 'webhost08',\n",
       " 'bellsuki1',\n",
       " 'lamborghin1',\n",
       " '123net123',\n",
       " 'p@sslng2diword',\n",
       " '52558000aaa',\n",
       " 'xanyrum650',\n",
       " 'byeypb2',\n",
       " 'examy624',\n",
       " 'sasuke4',\n",
       " 'tin030201',\n",
       " 'yuri110995',\n",
       " 'poseidon2011',\n",
       " 'barboza221294',\n",
       " 'clyioqzgw42',\n",
       " 'jr88072635',\n",
       " 'zcsntdmhe098',\n",
       " '1qa2ws3ed4rf',\n",
       " '52558000aaa',\n",
       " 'peluchin4',\n",
       " 'z888888',\n",
       " 'metopelo1623',\n",
       " 'sd6x9s3s',\n",
       " 'clave08',\n",
       " 'klara-tershina3H',\n",
       " 'spl51190595',\n",
       " 'control9',\n",
       " 'ubojig109',\n",
       " 'portales1',\n",
       " 'hpqkoxsn5',\n",
       " 'ass359',\n",
       " 'p@sslng2diword',\n",
       " 'denise18',\n",
       " 'kuntz80',\n",
       " 'intel1',\n",
       " 'elabadmin1386',\n",
       " 'owote852',\n",
       " 'khaled12',\n",
       " '10Erjrlmebup0n',\n",
       " 'finisterra1',\n",
       " 'lzhzad1989',\n",
       " '847XagYxUHUXOW',\n",
       " 'buqodym199',\n",
       " '1991vikash',\n",
       " 'amandine666',\n",
       " 'hola45',\n",
       " 'wycinu436',\n",
       " 'asdasdf1',\n",
       " 'hpqkoxsn5',\n",
       " 'beijing168',\n",
       " 'sw10d014',\n",
       " 'warriors08',\n",
       " 'barboza221294',\n",
       " '838188linh',\n",
       " 'bang6k',\n",
       " '2yz4ewwg',\n",
       " 'colorado27',\n",
       " 'hello2104',\n",
       " 'acgyj188',\n",
       " 'pazzini24',\n",
       " 'a2486315',\n",
       " 'cockw0mble',\n",
       " 'mike09',\n",
       " 'RPFUOUDQwMwVW0AS',\n",
       " 'sd6x9s3s',\n",
       " 's9830950044',\n",
       " 'purpledog1992',\n",
       " 'witek1709',\n",
       " '159951josh',\n",
       " 'juany57',\n",
       " '215466kenyi',\n",
       " 'p3rf3ct0',\n",
       " 'hayhayq2',\n",
       " 'terrassa6',\n",
       " 'zu20081965',\n",
       " 'nLIGyhTU1NQTAp6u',\n",
       " 'xp;ysmybst',\n",
       " 'exitos2009',\n",
       " 'urban1',\n",
       " 'kino3434',\n",
       " 'aio42fv',\n",
       " 'adminmao888',\n",
       " 'enziitoo1234',\n",
       " 'trabajonet9',\n",
       " 'just1n0k',\n",
       " 'nhfdff2512',\n",
       " 'sanjaime1',\n",
       " '283671gus',\n",
       " 'jonothepoop1',\n",
       " 'RqsuUsDYxNgr8T40',\n",
       " 'kXzWOozU2MQ1Jv1h',\n",
       " 'sw10d014',\n",
       " 'elonex24',\n",
       " '929865yt',\n",
       " 'folashade1',\n",
       " '4lgYVfzk1MwuzHcn',\n",
       " '248sUqiFEJuRag',\n",
       " 'portales1',\n",
       " 'qopybuxi2',\n",
       " 'xW8-3w7-MFB-CKH',\n",
       " '2d0d7qfz',\n",
       " 'vgnfs495vp',\n",
       " 'icap12',\n",
       " 'ass359',\n",
       " '0VKWoODkwOAc0pZK',\n",
       " 'metopelo1623',\n",
       " 'e667794c1d',\n",
       " 'tukaxo486',\n",
       " 'cyborged69',\n",
       " 'ocadezi586',\n",
       " 'kdl9cl53',\n",
       " 'khmer100.03278&?><Mnb',\n",
       " 'aqyba894',\n",
       " 'hpqkoxsn5',\n",
       " '1ngaymuadong',\n",
       " 'b9m7cxcgc',\n",
       " '0169395484a',\n",
       " 'ass359',\n",
       " 't8IkFRDIxMAFV2JW',\n",
       " 'kahcyxvj24',\n",
       " 'ixehawojEPe418',\n",
       " 'mathilde54550',\n",
       " 'zjl0kx03',\n",
       " '0169395484a',\n",
       " 'elonex24',\n",
       " 'josue12',\n",
       " 'aqyba894',\n",
       " 'aio42fv',\n",
       " '123477889a',\n",
       " 'clave2013',\n",
       " 'kVczcljg4OA25Aeb',\n",
       " 'kVczcljg4OA25Aeb',\n",
       " 'yy4129',\n",
       " 'lugerp08',\n",
       " '12345yolanda',\n",
       " 'a2531106',\n",
       " 'wuzyci421',\n",
       " 'llahetihw1',\n",
       " 'ykfums1',\n",
       " 'a2486315',\n",
       " 'demon10',\n",
       " 'legna13',\n",
       " 'jytifok873',\n",
       " 'desmondkok21',\n",
       " 'exitos2009',\n",
       " 'kenneth610',\n",
       " '20010509wang',\n",
       " 'acetita478',\n",
       " 'okn9zp9o',\n",
       " 'iL1BEmTUyMg8YYbn',\n",
       " 'wxS2ztDk4OATjBfI',\n",
       " '19821010a',\n",
       " 'trabajonet9',\n",
       " 'weicat12',\n",
       " 'polo2014',\n",
       " 'metopelo1623',\n",
       " 'a0972986650',\n",
       " 'v10rica',\n",
       " 'Herzberg@ABBOTT33656888commerce',\n",
       " 'onurb1994',\n",
       " 'hamqrc6',\n",
       " 'z7zbgIDkzMQeHUd9',\n",
       " 'puegwajy416',\n",
       " 'pikey231',\n",
       " 'wasanun13',\n",
       " '1234159hero',\n",
       " '1234159hero',\n",
       " 'v1118714',\n",
       " 'ayles2266',\n",
       " 'hamqrc6',\n",
       " 'fk9qi21m',\n",
       " 'gtlek',\n",
       " 'schalke04',\n",
       " 'sandra0547',\n",
       " 'onurb1994',\n",
       " 'kenneth610',\n",
       " 'galoucura1',\n",
       " 'cUFUSYKIPuGo024',\n",
       " 'J0LcDWDc2NAVE8j3',\n",
       " 'icap12',\n",
       " 'webstudio8',\n",
       " 'acetita478',\n",
       " 'pHyqueDIyNQ8vmhb',\n",
       " 'juany57',\n",
       " '12345yolanda',\n",
       " 'iubat09',\n",
       " 'skoda06',\n",
       " 'AVYq1lDE4MgAZfNt',\n",
       " 'taulant123',\n",
       " 'BsKbJHTY4NgesCOs',\n",
       " 'ydd45ee',\n",
       " '01161590m',\n",
       " 'synyxyr723',\n",
       " '20Dgw7TQ0OQVdly7',\n",
       " 'BsKbJHTY4NgesCOs',\n",
       " 'warriors08',\n",
       " '123maxbala',\n",
       " 'Oshity07142014',\n",
       " 'tim80327',\n",
       " 'nK0yKXTU0NQHZE2e',\n",
       " 'k1k2k3k4k5k6',\n",
       " 'ekufite742',\n",
       " 'ok>bdk',\n",
       " 'il0vey0u',\n",
       " 'oscar2002',\n",
       " 'snolyuj04',\n",
       " 'yuri110995',\n",
       " 'mustang337',\n",
       " 'rLLh4WDQ2OAWbDO5',\n",
       " '05bumd',\n",
       " 'patata91',\n",
       " 'hola45',\n",
       " 'bgrvl80',\n",
       " 'vijay1995',\n",
       " 'graciela2',\n",
       " 'mayur@8netinfotech',\n",
       " 'elyass15@ajilent-ci',\n",
       " 'nK0yKXTU0NQHZE2e',\n",
       " 'jntjmh7',\n",
       " 'Ju6BIMTU0MwYXtL4',\n",
       " 'bugatti01',\n",
       " 'vestax25',\n",
       " 'mussuh4',\n",
       " 'PEPITO00',\n",
       " 'z3ro1sm',\n",
       " 'asdasdf1',\n",
       " 'marita1',\n",
       " 'ezekiel720',\n",
       " 'taurofive16',\n",
       " 'jerusalem393',\n",
       " 'cigicigi123',\n",
       " '090mca090',\n",
       " 'tukaxo486',\n",
       " 'puegwajy416',\n",
       " 'Zdyf0kjMzNQycqPx',\n",
       " 'falre1524',\n",
       " 'regodib479',\n",
       " 'kinga22',\n",
       " 'ab11223344',\n",
       " 'popo0404',\n",
       " 'faranumar91',\n",
       " 'muoaqxwc21',\n",
       " 'lqksuym982',\n",
       " 'arum210684',\n",
       " 'gerardway1',\n",
       " 'x0004534',\n",
       " 'gohan2602',\n",
       " '4fqa52vecr',\n",
       " '23deagosto',\n",
       " '147963asd',\n",
       " 'olmaz.',\n",
       " 'yogesh143',\n",
       " 'wearehis7',\n",
       " '52558000aaa',\n",
       " '4165000yakub',\n",
       " 'vocal0619',\n",
       " 'khaled12',\n",
       " 'yogesh143',\n",
       " 'jekkmoeder>',\n",
       " 'j03l4ytr1',\n",
       " 'failz0r',\n",
       " 'teste10',\n",
       " '6975038lp',\n",
       " 'hasan18',\n",
       " 'p@sslng2diword',\n",
       " 'calcifer32',\n",
       " 'teemteem97',\n",
       " 'lollies1989',\n",
       " 'Truelove19902610',\n",
       " 'elperro1',\n",
       " 'rqmswof2llb0',\n",
       " 'gozv3e5',\n",
       " 'labarge1',\n",
       " 'tamanagung6',\n",
       " 'kikeq102',\n",
       " 'han19660120',\n",
       " '6yy6yy',\n",
       " 'jbiz04h4',\n",
       " 'vardhan19',\n",
       " 'tPGMkBjkyMg3hGzu',\n",
       " 'trabajonet9',\n",
       " 'nikolas369',\n",
       " 'novelia21',\n",
       " 'aqyba894',\n",
       " 'junaid5',\n",
       " 'ts34a3fodh3i',\n",
       " '26522876p',\n",
       " 'pmcm110118008',\n",
       " 'qopybuxi2',\n",
       " '631ihOZogELoVap',\n",
       " 'skoda06',\n",
       " '631ihOZogELoVap',\n",
       " 'ineedyou23',\n",
       " 'viri13',\n",
       " 'sarahi1628',\n",
       " 'bgrvl80',\n",
       " 'peluchin4',\n",
       " 'topgan22',\n",
       " '283671gus',\n",
       " 'aslpls2009',\n",
       " 'legna13',\n",
       " 'papasito1991',\n",
       " 'josef0867',\n",
       " 'in595462',\n",
       " 'kciseba24521',\n",
       " 'b9m7cxcgc',\n",
       " 'memjan123',\n",
       " 'walterivl13',\n",
       " '101010hadis',\n",
       " '12345yolanda',\n",
       " 'krishna2',\n",
       " 'JEQuloqOFUd102',\n",
       " 'wxS2ztDk4OATjBfI',\n",
       " '19840510kkk1',\n",
       " 'la3na4you',\n",
       " 'mzhrmir786',\n",
       " 'aa123000',\n",
       " 'barboza221294',\n",
       " 'wearehis7',\n",
       " '2863e00016',\n",
       " 'speedracer10',\n",
       " 'AS0130066',\n",
       " 'teste10',\n",
       " 'kukimuki123',\n",
       " 'gohan2602',\n",
       " '3f5xd41l0ik7',\n",
       " 'bencike7',\n",
       " 'intel1',\n",
       " 'b9m7cxcgc',\n",
       " '52558000aaa',\n",
       " '52756652a',\n",
       " 'g067057895',\n",
       " 'GGmm26120904..',\n",
       " 'tin030201',\n",
       " 'stalucia66',\n",
       " 'xve33ea',\n",
       " 'elperro1',\n",
       " 'koulapic33',\n",
       " 'trabajonet9',\n",
       " 'a0972986650',\n",
       " 'lollies1989',\n",
       " 'wjngzro27',\n",
       " 'as8594505',\n",
       " 'cerner09',\n",
       " 'servbot88',\n",
       " 'wibi182d',\n",
       " 'muoaqxwc21',\n",
       " 'uuuu123',\n",
       " 'matiofox08',\n",
       " 'coy29061994',\n",
       " 'damyvo114',\n",
       " '030005qw',\n",
       " '3vszncp4',\n",
       " '1ngaymuadong',\n",
       " '1justogax',\n",
       " 'meopvywk628',\n",
       " 'rrilni1747',\n",
       " '746xitEGiqObog',\n",
       " 'luthien123',\n",
       " 'padhila30',\n",
       " 'urban1',\n",
       " 'jytifok873',\n",
       " 'lovelory1',\n",
       " 'uxyloga692',\n",
       " 'webhost0hm000',\n",
       " 'lamborghin1',\n",
       " 'alimagik1',\n",
       " 'ebogel225',\n",
       " 'nikolas369',\n",
       " 'eth36498',\n",
       " 'ykfums1',\n",
       " 'pastorius88',\n",
       " 'afavin964',\n",
       " 'diegote10',\n",
       " 'Truelove19902610',\n",
       " '01161590m',\n",
       " 'Truelove19902610',\n",
       " '5874813o',\n",
       " 'popo0404',\n",
       " 'trust123',\n",
       " 'fnmsdha476',\n",
       " 'umetic21',\n",
       " 'kswa2mrv',\n",
       " 'qopybuxi2',\n",
       " 'spl51190595',\n",
       " 'frenchtoast42',\n",
       " 'carla99',\n",
       " 'asgaliu11',\n",
       " 'p0lp0l',\n",
       " 'ram@!sita15392',\n",
       " 'juliana00',\n",
       " 'juanito00',\n",
       " 'mtvwyz001',\n",
       " 'RqsuUsDYxNgr8T40',\n",
       " 'patata91',\n",
       " 'visi7k1yr',\n",
       " 'bjolgvhs69',\n",
       " 's4m2dx9e6',\n",
       " 'gpc151192',\n",
       " 'uzifyc502',\n",
       " 'amoadios321',\n",
       " '17.10.08',\n",
       " 'zb08110229',\n",
       " 'z7zbgIDkzMQeHUd9',\n",
       " 'vmdo3i',\n",
       " 'vgnfs495vp',\n",
       " 'bghuyku37',\n",
       " 'adfbwrnt4',\n",
       " 'groster152',\n",
       " 'bugatti01',\n",
       " 'terrassa6',\n",
       " 'poluxyj32',\n",
       " 'franczuk33',\n",
       " 'hamqrc6',\n",
       " 'mmm23mm',\n",
       " 'savas123x',\n",
       " 'jkprm45',\n",
       " 'denise18',\n",
       " 'patty94',\n",
       " '1w2e3s4l5e6y',\n",
       " 'jj46azbo',\n",
       " 'cockw0mble',\n",
       " 'hot622204',\n",
       " 'faisal213',\n",
       " '8g8x2su3',\n",
       " 'housefly74',\n",
       " 'pardalgg5',\n",
       " 'p3rf3ct0',\n",
       " '05bumd',\n",
       " 'yk530mg8',\n",
       " 'ajyrew547',\n",
       " 'pazzini24',\n",
       " '090mca090',\n",
       " 'taurofive16',\n",
       " 'studenko123',\n",
       " 'LypOJUfuLYrO477',\n",
       " 'daaxvie1',\n",
       " 'we34dar88',\n",
       " 'markama10',\n",
       " 'senghong2009',\n",
       " 'owary200',\n",
       " 'hayhayq2',\n",
       " 'frenchtoast42',\n",
       " 'wycinu436',\n",
       " 'ihana906',\n",
       " 'carla99',\n",
       " 'polo2014',\n",
       " 'oioo9og',\n",
       " '1234159hero',\n",
       " '147963asd',\n",
       " 'chiefwanker1',\n",
       " 'pHyqueDIyNQ8vmhb',\n",
       " 'uuuu123',\n",
       " 'meopvywk628',\n",
       " 'cacat123',\n",
       " '20Dgw7TQ0OQVdly7',\n",
       " '3vszncp4',\n",
       " 'gaymaids1',\n",
       " 'xie635891',\n",
       " 'hamqrc6',\n",
       " 'aqyba894',\n",
       " 'naseKoBUMIg295',\n",
       " 'njmania114',\n",
       " 'faranumar91',\n",
       " 'lugerp08',\n",
       " 'khaled12',\n",
       " 'BB11227122',\n",
       " 'linhna288',\n",
       " 'SLAEgyTk0OQxphJq',\n",
       " 'hisnipes1',\n",
       " 'jntjmh7',\n",
       " 'sucupi516',\n",
       " 'mega0109',\n",
       " 'tollak123',\n",
       " 'hpw1907v',\n",
       " 'mega0109',\n",
       " 'bagdas2011',\n",
       " '12345687vini',\n",
       " 'asgaliu11',\n",
       " 'a03242241431a',\n",
       " 'laedbchsx687',\n",
       " 'intel1',\n",
       " 'z888888',\n",
       " 'orzwnyj91',\n",
       " 'X34y2CzY5MACs6kp',\n",
       " 'rqmswof2llb0',\n",
       " 'aquhih220',\n",
       " 'saule123',\n",
       " 'killer5',\n",
       " 'kP82iqDMxNgBMxBP',\n",
       " 'clumsy0619',\n",
       " 'vgnfs495vp',\n",
       " 'korea2010',\n",
       " '13bola',\n",
       " 'iwaguh884',\n",
       " 'mickael12',\n",
       " '1qa2ws3ed4r',\n",
       " 'lofebop480',\n",
       " '33kanun03',\n",
       " 'jorge1489',\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. NLP + ML Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5a. Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF to store all characters in each password\n",
    "def word_divide_char(inputs):\n",
    "    characters = []\n",
    "    for i in inputs:\n",
    "        characters.append(i)\n",
    "    return characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(tokenizer=word_divide_char)\n",
    "X = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(669639, 133)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have created a dataset of vectors for each character in the passwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k': 62,\n",
       " 'z': 77,\n",
       " 'd': 55,\n",
       " 'e': 56,\n",
       " '5': 35,\n",
       " '7': 37,\n",
       " 'i': 60,\n",
       " 'n': 65,\n",
       " 'o': 66,\n",
       " '3': 33,\n",
       " '4': 34,\n",
       " 'm': 64,\n",
       " 'g': 58,\n",
       " 'y': 76,\n",
       " '1': 31,\n",
       " '2': 32,\n",
       " 'a': 52,\n",
       " 'v': 73,\n",
       " 'q': 68,\n",
       " 'l': 63,\n",
       " 'f': 57,\n",
       " 't': 71,\n",
       " 's': 70,\n",
       " 'r': 69,\n",
       " '8': 38,\n",
       " '6': 36,\n",
       " '0': 30,\n",
       " 'b': 53,\n",
       " 'h': 59,\n",
       " '9': 39,\n",
       " 'u': 72,\n",
       " 'c': 54,\n",
       " 'w': 74,\n",
       " 'p': 67,\n",
       " 'j': 61,\n",
       " 'x': 75,\n",
       " '>': 43,\n",
       " '@': 45,\n",
       " '-': 27,\n",
       " '!': 17,\n",
       " '.': 28,\n",
       " '_': 50,\n",
       " ';': 40,\n",
       " '&': 22,\n",
       " '?': 44,\n",
       " '<': 41,\n",
       " ' ': 16,\n",
       " '$': 20,\n",
       " '\"': 18,\n",
       " '~': 81,\n",
       " '+': 26,\n",
       " '^': 49,\n",
       " '/': 29,\n",
       " ')': 24,\n",
       " '#': 19,\n",
       " '%': 21,\n",
       " '*': 25,\n",
       " '(': 23,\n",
       " '': 124,\n",
       " '[': 46,\n",
       " ']': 48,\n",
       " '\\\\': 47,\n",
       " '`': 51,\n",
       " '': 128,\n",
       " '': 120,\n",
       " '\\x1c': 14,\n",
       " '': 94,\n",
       " '=': 42,\n",
       " '{': 78,\n",
       " '': 121,\n",
       " '}': 80,\n",
       " '\\x16': 10,\n",
       " '': 119,\n",
       " '': 100,\n",
       " '\\x1e': 15,\n",
       " '': 96,\n",
       " '\\x19': 12,\n",
       " '': 110,\n",
       " '\\x7f': 82,\n",
       " '': 93,\n",
       " '': 122,\n",
       " '': 103,\n",
       " '|': 79,\n",
       " '': 107,\n",
       " '': 108,\n",
       " '': 102,\n",
       " '': 86,\n",
       " '': 111,\n",
       " '': 129,\n",
       " '': 97,\n",
       " '\\x05': 2,\n",
       " '\\x1b': 13,\n",
       " '': 106,\n",
       " '': 131,\n",
       " '': 123,\n",
       " '': 117,\n",
       " '': 104,\n",
       " '\\x10': 7,\n",
       " '\\x17': 11,\n",
       " '': 125,\n",
       " '': 109,\n",
       " '': 127,\n",
       " '': 116,\n",
       " '': 91,\n",
       " '': 95,\n",
       " '': 118,\n",
       " '\\x81': 83,\n",
       " '': 98,\n",
       " '\\x08': 4,\n",
       " '': 115,\n",
       " '': 112,\n",
       " '': 132,\n",
       " '\\x12': 9,\n",
       " '': 101,\n",
       " '': 99,\n",
       " '': 89,\n",
       " '\\x0f': 6,\n",
       " '': 130,\n",
       " '': 90,\n",
       " '': 114,\n",
       " '\\x06': 3,\n",
       " '\\xa0': 85,\n",
       " '': 87,\n",
       " '\\x04': 1,\n",
       " '\\x0e': 5,\n",
       " '\\x8d': 84,\n",
       " '': 113,\n",
       " '': 126,\n",
       " '\\x11': 8,\n",
       " '': 92,\n",
       " '\\x01': 0,\n",
       " '': 88,\n",
       " '': 105}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print vocabulary of the vectors\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us explore the tfidf values of the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kzde5577'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take the first record for instance\n",
    "df.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.591282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.567515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>z</td>\n",
       "      <td>0.335766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>k</td>\n",
       "      <td>0.291844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>d</td>\n",
       "      <td>0.285376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>e</td>\n",
       "      <td>0.221274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\u0001</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>~</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>}</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>|</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>{</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>y</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>x</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>w</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>v</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>u</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>r</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>s</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>o</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>q</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>/</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>.</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>-</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>+</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>*</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>)</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&amp;</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>%</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>$</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>#</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\"</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>!</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\u001b</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\u0019</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\u0017</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\u0016</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\u0012</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\u0011</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\u0010</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\u000f</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\u000e</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\b</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\u0006</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\u0005</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>p</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>_</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\u0004</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>n</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>m</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>l</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>j</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>i</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>h</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>g</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>f</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>c</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>b</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>a</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>`</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>^</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>]</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\\</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>@</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>?</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&gt;</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>=</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>;</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tfidf\n",
       "7  0.591282\n",
       "5  0.567515\n",
       "z  0.335766\n",
       "k  0.291844\n",
       "d  0.285376\n",
       "e  0.221274\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "\u0001  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "~  0.000000\n",
       "}  0.000000\n",
       "|  0.000000\n",
       "{  0.000000\n",
       "y  0.000000\n",
       "x  0.000000\n",
       "w  0.000000\n",
       "v  0.000000\n",
       "u  0.000000\n",
       "t  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "r  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "  0.000000\n",
       "s  0.000000\n",
       "o  0.000000\n",
       "q  0.000000\n",
       "   0.000000\n",
       "/  0.000000\n",
       ".  0.000000\n",
       "-  0.000000\n",
       "+  0.000000\n",
       "*  0.000000\n",
       ")  0.000000\n",
       "(  0.000000\n",
       "&  0.000000\n",
       "%  0.000000\n",
       "$  0.000000\n",
       "#  0.000000\n",
       "\"  0.000000\n",
       "!  0.000000\n",
       "\n",
       "  0.000000\n",
       "1  0.000000\n",
       "\n",
       "  0.000000\n",
       "\u001b  0.000000\n",
       "\u0019  0.000000\n",
       "\u0017  0.000000\n",
       "\u0016  0.000000\n",
       "\u0012  0.000000\n",
       "\u0011  0.000000\n",
       "\u0010  0.000000\n",
       "\u000f  0.000000\n",
       "\u000e  0.000000\n",
       "\b  0.000000\n",
       "\u0006  0.000000\n",
       "\u0005  0.000000\n",
       "0  0.000000\n",
       "2  0.000000\n",
       "p  0.000000\n",
       "_  0.000000\n",
       "\u0004  0.000000\n",
       "n  0.000000\n",
       "m  0.000000\n",
       "l  0.000000\n",
       "j  0.000000\n",
       "i  0.000000\n",
       "h  0.000000\n",
       "g  0.000000\n",
       "f  0.000000\n",
       "c  0.000000\n",
       "b  0.000000\n",
       "a  0.000000\n",
       "`  0.000000\n",
       "^  0.000000\n",
       "3  0.000000\n",
       "]  0.000000\n",
       "\\  0.000000\n",
       "[  0.000000\n",
       "@  0.000000\n",
       "?  0.000000\n",
       ">  0.000000\n",
       "=  0.000000\n",
       "<  0.000000\n",
       ";  0.000000\n",
       "9  0.000000\n",
       "8  0.000000\n",
       "6  0.000000\n",
       "4  0.000000\n",
       "  0.000000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# get tfidf vector for first document\n",
    "first_document_vector = X[0]\n",
    "\n",
    "# print the scores\n",
    "data = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n",
    "data.sort_values(by=[\"tfidf\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logistic regression\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-vs-Rest (OvR) Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'ovr' = one-vs-rest\n",
    "# initialise LogisticRegression with parameters to make it multi-class classifier\n",
    "log_clf_ovr = LogisticRegression(penalty='l2', multi_class='ovr')  # initi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model training\n",
    "log_clf_ovr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8128696015769667\n"
     ]
    }
   ],
   "source": [
    "# model evaluation\n",
    "print(log_clf_ovr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 81% score of the OVR Logistic Regression classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_class='multinomial'\n",
    "log_clf_multi = LogisticRegression(random_state=0, multi_class='multinomial', solver='newton-cg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='newton-cg', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model training\n",
    "log_clf_multi.fit(X_train, y_train)  # train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8196344304402365\n"
     ]
    }
   ],
   "source": [
    "# model evaluation\n",
    "print(log_clf_multi.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Scores\n",
    "- Logistic Regression Score : 81.96%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prediction - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF to predict password strength\n",
    "def pwd_predict(prediction):\n",
    "    if prediction == 0:\n",
    "        return \"Weak Password\"\n",
    "    elif prediction == 1:\n",
    "        return \"Strong Password\"\n",
    "    else:\n",
    "        return \"Super Strong Password\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-vs-Rest Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Weak Password'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_predict = np.array([\"abcd\"])\n",
    "X_predict = vectorizer.transform(X_predict)\n",
    "y_pred = log_clf_ovr.predict(X_predict)\n",
    "pwd_predict(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Strong Password'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_predict = np.array([\"@123\"])\n",
    "X_predict = vectorizer.transform(X_predict)\n",
    "y_pred = log_clf_ovr.predict(X_predict)\n",
    "pwd_predict(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Super Strong Password'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_predict = np.array([\"Utkarsh@123\"])\n",
    "X_predict = vectorizer.transform(X_predict)\n",
    "y_pred = log_clf_ovr.predict(X_predict)\n",
    "pwd_predict(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Weak Password'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_predict = np.array([\"abc\"])\n",
    "X_predict = vectorizer.transform(X_predict)\n",
    "y_pred = log_clf_multi.predict(X_predict)\n",
    "pwd_predict(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Strong Password'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_predict = np.array([\"Utkarsh123\"])\n",
    "X_predict = vectorizer.transform(X_predict)\n",
    "y_pred = log_clf_multi.predict(X_predict)\n",
    "pwd_predict(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Super Strong Password'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_predict = np.array([\"Utkarsh@123\"])\n",
    "X_predict = vectorizer.transform(X_predict)\n",
    "y_pred = log_clf_multi.predict(X_predict)\n",
    "pwd_predict(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5d. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/49/b95c037b717b4ceadc76b6e164603471225c27052d1611d5a2e832757945/xgboost-0.90-py2.py3-none-win_amd64.whl (18.3MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from xgboost) (1.16.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from xgboost) (1.3.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-0.90\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import XGBoost\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "              nthread=None, objective='multi:softprob', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialise XGBClassifier\n",
    "xgb_classifier = xgb.XGBClassifier()\n",
    "# train model\n",
    "xgb_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9150513708858491"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xgb model evaluation\n",
    "xgb_classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Scores\n",
    "- Logistic Regression Score : 81.96%\n",
    "- XGBClassifier Score       : 91.51%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prediction : XGB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Weak Password'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_predict = np.array([\"abc\"])\n",
    "X_predict = vectorizer.transform(X_predict)\n",
    "y_pred = xgb_classifier.predict(X_predict)\n",
    "# print(y_pred)\n",
    "pwd_predict(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Strong Password'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_predict = np.array([\"123@abc\"])\n",
    "X_predict = vectorizer.transform(X_predict)\n",
    "y_pred = xgb_classifier.predict(X_predict)\n",
    "pwd_predict(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Strong Password'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_predict = np.array([\"Utkarsh@123\"])\n",
    "X_predict = vectorizer.transform(X_predict)\n",
    "y_pred = xgb_classifier.predict(X_predict)\n",
    "pwd_predict(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5e. Multinomial Naive Bayes\n",
    "- Generally used for text based processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import MultinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize MultinomialNB classifier\n",
    "nb_classifier= MultinomialNB()\n",
    "# train model\n",
    "nb_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7436533062541066"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model evaluation\n",
    "nb_classifier.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Scores\n",
    "- Logistic Regression Score : 81.96%\n",
    "- XGBClassifier Score       : 91.51%\n",
    "- Multinomial Naive Bayes : 74.36%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prediction : MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Strong Password'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_predict = np.array([\"abc\"])\n",
    "X_predict = vectorizer.transform(X_predict)\n",
    "y_pred = nb_classifier.predict(X_predict)\n",
    "# print(y_pred)\n",
    "pwd_predict(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model is not predicting well, therefore, we will not use this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "- XGBoost performed the best for predicting the password strength with 91% accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
